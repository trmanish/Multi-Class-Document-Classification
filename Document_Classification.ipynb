{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does Multi-Class Classification on 20 sets of Documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import scipy.sparse as sp\n",
    "%matplotlib inline  \n",
    "# Don't use plt.show() as it opens a new window and blocks the evaluation of cell. \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing the default data frame options. \n",
    "pd.set_option(\"max_r\",3000) # Set the default rows to 80\n",
    "pd.set_option(\"max_columns\",51000) # Set the default columns to 500 from 20. \n",
    "pd.set_option(\"expand_frame_repr\",False) # Checks if frame can be expanded or truncated. Make it expand column-wise\n",
    "np.set_printoptions(threshold=np.inf) # Removes the threshold level to print numpy array\n",
    "np.set_printoptions(suppress=True) # Prevent printing in scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multiclass Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset from different folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This funtion reads in the data from different documents and stores as list\n",
    "\n",
    "def readData():\n",
    "    \"\"\"This fucntion reads the document files from different sub-directories\"\"\"\n",
    "    docs_list=[]\n",
    "    target_class=[]\n",
    "    target_recoded=[]\n",
    "    x=-1\n",
    "    for (dirnames, dir,files) in os.walk('./data'):\n",
    "        if (x<0):\n",
    "            class_names=dir\n",
    "            x+=1\n",
    "            continue\n",
    "        for y in range(len(files)):\n",
    "            docs_list.append(open(dirnames+'/'+files[y]).read())\n",
    "            target_class.append(dirnames[7:])\n",
    "            target_recoded.append(x)    # Getting the target class as numeric codes: 0 for 1st class, 1 for 2nd and so on..\n",
    "        x+=1\n",
    "    \n",
    "    # Taking some sense of data\n",
    "    print(\"Total no. of documents:\",len(target_class))\n",
    "    print(\"Total Classes:\", len(class_names))\n",
    "    print(\"Average no. of documents in a class\",len(target_class)/len(class_names))\n",
    "    return docs_list,target_recoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training and test split\n",
    "\n",
    "def split(docs_list,target_recoded):\n",
    "    \"\"\"This function samples the dataset into training and testing\"\"\"\n",
    "    # Splitting into training and test. \n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    train_X, test_X,train_Y,test_Y = train_test_split(docs_list, target_recoded, test_size=0.30, random_state=42)\n",
    "    \n",
    "    return train_X, test_X,train_Y,test_Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cleaning the doc files\n",
    "\n",
    "def cleaningDocs(doc,stem='L'):  # 'S' for Stemming, 'L' for Lemmatization\n",
    "    \"\"\"This function cleans each doc string by doing the following: \n",
    "    i)   Removing punctuation and other non alphabetical characters\n",
    "    ii)  Convert to Lower case and split string into words (tokenization)\n",
    "    ii)  Removes stop words (most frequent words)\n",
    "    iii) Doing Stemming and Lemmatization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing punctuations and other non alphabetic characters\n",
    "    import re\n",
    "    alphabets_only=re.sub(r'[^a-zA-Z]',\" \",doc)\n",
    "    \n",
    "    # Converting to lower case and splitting the words(tokenization)\n",
    "    words_lower=alphabets_only.lower().split()\n",
    "    \n",
    "    # Removing stop words (Words like 'a', 'an', 'is','the' which doesn't contribute anything\n",
    "    from nltk.corpus import stopwords\n",
    "    useful_words = [w for w in words_lower if not w in set(stopwords.words(\"english\"))] \n",
    "    \n",
    "    # Doing Stemming or Lemmatization (Normalising the text)\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    if (stem=='S'):  # Choosing between Stemming ('S') and Lemmatization ('L')\n",
    "        stemmer=PorterStemmer()\n",
    "        final_words=[stemmer.stem(x) for x in useful_words]\n",
    "    else: \n",
    "        lemma=WordNetLemmatizer()\n",
    "        final_words=[lemma.lemmatize(x) for x in useful_words]\n",
    "        \n",
    "    \n",
    "    return( \" \".join(final_words))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def processing(file,stem=\"L\"):\n",
    "    \"\"\" Function to clean the training or test docs\n",
    "    Pass the name of file as argument to be cleaned\n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_train_X=[]\n",
    "    for x in file: \n",
    "        cleaned_train_X.append(cleaningDocs(x,stem))\n",
    "    return cleaned_train_X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating Bag of words feature vectors for each document\n",
    "\n",
    "def bagofWords(X,Y,max_feature=5000,type=\"count\"):\n",
    "    \"\"\"This function creates a Bag of Features vectors from the original documents\"\"\"\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    \n",
    "    if(type==\"count\"): # To choose between count or tf-idf bag or words model\n",
    "        vectorizer = CountVectorizer(analyzer = \"word\",max_features = max_feature) \n",
    "    else: \n",
    "        vectorizer = TfidfVectorizer(analyzer = \"word\",max_features = max_feature)\n",
    "        \n",
    "    X=vectorizer.fit_transform(X)\n",
    "    return X ,np.array(Y),vectorizer # Converting to numpy array\n",
    "\n",
    "\n",
    "    \n",
    "class learner(object):\n",
    "    \"\"\"Creating a class to efficiently run multiple algorithms on the same dataset\"\"\"\n",
    "    \n",
    "    def __init__(self,train_X,train_Y,k=5):\n",
    "        self.k=k\n",
    "        self.train_X=train_X\n",
    "        self.train_Y=train_Y\n",
    "\n",
    "        \n",
    "    # Running algorithm with 5 fold cross-validation\n",
    "\n",
    "    def kFold(self):\n",
    "        '''This fucntion splits the training set into k folds\n",
    "        '''\n",
    "        from sklearn import cross_validation\n",
    "        self.k_fold=cross_validation.KFold(n=self.train_X.shape[0],n_folds=self.k)\n",
    "    \n",
    "\n",
    "            \n",
    "    def GNB(self):\n",
    "        \"\"\"Method to implement Multi-class Gaussian Naive Bayes\"\"\"\n",
    "        \n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        scores_gnb = []\n",
    "        \n",
    "        for train_indices, test_indices in self.k_fold:\n",
    "            train_X_cv = self.train_X[train_indices].todense()\n",
    "            train_Y_cv= self.train_Y[train_indices]\n",
    " \n",
    "            test_X_cv = self.train_X[test_indices].todense()\n",
    "            test_Y_cv= self.train_Y[test_indices]\n",
    "        \n",
    "            self.gnb=GaussianNB()\n",
    "            scores_gnb.append(self.gnb.fit(train_X_cv,train_Y_cv).score(test_X_cv,test_Y_cv))\n",
    "            \n",
    "        print(\"The mean accuracy of GaussianNaive Bayes on CV data is:\", np.mean(scores_gnb))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def RF(self):\n",
    "        \"\"\"Method to implement Multi-class RandomForest\"\"\"\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        scores_rf = []\n",
    "        \n",
    "        for train_indices, test_indices in self.k_fold:\n",
    "            train_X_cv = self.train_X[train_indices].todense()\n",
    "            train_Y_cv= self.train_Y[train_indices]\n",
    " \n",
    "            test_X_cv = self.train_X[test_indices].todense()\n",
    "            test_Y_cv= self.train_Y[test_indices]\n",
    "        \n",
    "            self.rf=RandomForestClassifier(n_estimators=150,criterion='entropy')\n",
    "            scores_rf.append(self.rf.fit(train_X_cv,train_Y_cv).score(test_X_cv,test_Y_cv))\n",
    "\n",
    "        print(\"The mean accuracy of Random Forests on CV data is:\", np.mean(scores_rf))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def SGD(self):\n",
    "        \"\"\"Method to implement Multi-class SVM using Stochastic Gradient Descent\"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        scores_sgd = []\n",
    "        \n",
    "        for train_indices, test_indices in self.k_fold:\n",
    "            train_X_cv = self.train_X[train_indices].todense()\n",
    "            train_Y_cv= self.train_Y[train_indices]\n",
    " \n",
    "            test_X_cv = self.train_X[test_indices].todense()\n",
    "            test_Y_cv= self.train_Y[test_indices]\n",
    "        \n",
    "            self.sgd=SGDClassifier(loss='hinge',penalty='l2')\n",
    "            scores_sgd.append(self.sgd.fit(train_X_cv,train_Y_cv).score(test_X_cv,test_Y_cv))\n",
    "\n",
    "        print(\"The mean accuracy of Stochastic Gradient Descent Classifier on CV data is:\", np.mean(scores_sgd))   \n",
    "        \n",
    "        \n",
    "        \n",
    "    def test_performance(self,test_X,test_Y):\n",
    "        \"\"\"This method checks the performance of each algorithm on test data.\"\"\"\n",
    "        \n",
    "        from sklearn import metrics\n",
    "        \n",
    "        # For GNB\n",
    "        print (\"The accuracy of GNB on test data is:\", self.gnb.score(test_X,test_Y))\n",
    "        print 'Classification Metrics for GNB'\n",
    "        print metrics.classification_report(test_Y, self.gnb.predict(test_X))\n",
    "        print \"Confusion matrix\"\n",
    "        print metrics.confusion_matrix(test_Y, self.gnb.predict(test_X))\n",
    "        \n",
    "        # For RandomForest\n",
    "        print (\"The accuracy of Random Forest on test data is:\", self.rf.score(test_X,test_Y))\n",
    "        print 'Classification Metrics for RandomForest'\n",
    "        print metrics.classification_report(test_Y, self.rf.predict(test_X))\n",
    "        print \"Confusion matrix\"\n",
    "        print metrics.confusion_matrix(test_Y, self.rf.predict(test_X))\n",
    "        \n",
    "        # For SGD\n",
    "        print (\"The accuracy of SGD on test data is:\", self.sgd.score(test_X,test_Y))\n",
    "        print 'Classification Metrics for SGD'\n",
    "        print metrics.classification_report(test_Y, self.sgd.predict(test_X))\n",
    "        print \"Confusion matrix\"\n",
    "        print metrics.confusion_matrix(test_Y, self.sgd.predict(test_X))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning original docs by doing text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total no. of documents:', 18834)\n",
      "('Total Classes:', 20)\n",
      "('Average no. of documents in a class', 941)\n"
     ]
    }
   ],
   "source": [
    "# Reading Datasets\n",
    "docs_list,target_Y=readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into training and test\n",
    "train_X, test_X,train_Y,test_Y=split(docs_list,target_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning the training docs files\n",
    "cleaned_train_X=processing(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating bag of words\n",
    "train_X, train_Y,vectorizer=bagofWords(cleaned_train_X,train_Y,type=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13183, 5000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Runnning the algorithms\n",
    "obj=learner(train_X,train_Y)\n",
    "obj.kFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The mean accuracy of GaussianNaive Bayes on CV data is:', 0.6906625280601778)\n"
     ]
    }
   ],
   "source": [
    "# Running Gaussian Naive Bayes\n",
    "obj.GNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The mean accuracy of Random Forests on CV data is:', 0.76158766658437793)\n"
     ]
    }
   ],
   "source": [
    "#Running Random Forests\n",
    "obj.RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The mean accuracy of Stochastic Gradient Descent Classifier on CV data is:', 0.85739335693812158)\n"
     ]
    }
   ],
   "source": [
    "# Running Stochastic Gradient Descent SVM\n",
    "obj.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting the test data into Bags of model. \n",
    "\n",
    "# Cleaning the training docs files\n",
    "test_X=processing(test_X)\n",
    "# Creating bag of words\n",
    "test_X=vectorizer.transform(test_X)\n",
    "test_Y=np.array(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The accuracy of GNB on test data is:', 0.68837373916121036)\n",
      "Classification Metrics for GNB\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.77      0.77       220\n",
      "          1       0.52      0.55      0.53       303\n",
      "          2       0.52      0.46      0.49       280\n",
      "          3       0.53      0.54      0.53       286\n",
      "          4       0.60      0.58      0.59       275\n",
      "          5       0.68      0.61      0.64       299\n",
      "          6       0.62      0.48      0.54       302\n",
      "          7       0.70      0.62      0.66       293\n",
      "          8       0.86      0.71      0.78       329\n",
      "          9       0.88      0.77      0.82       305\n",
      "         10       0.91      0.88      0.90       310\n",
      "         11       0.74      0.90      0.81       279\n",
      "         12       0.59      0.54      0.57       310\n",
      "         13       0.68      0.71      0.70       309\n",
      "         14       0.66      0.84      0.74       294\n",
      "         15       0.79      0.82      0.80       306\n",
      "         16       0.62      0.78      0.69       254\n",
      "         17       0.85      0.92      0.89       292\n",
      "         18       0.57      0.77      0.65       209\n",
      "         19       0.67      0.48      0.56       196\n",
      "\n",
      "avg / total       0.69      0.69      0.69      5651\n",
      "\n",
      "Confusion matrix\n",
      "[[170   0   0   0   0   0   0   1   0   1   1   5   0   1   1  15   1   4\n",
      "    6  14]\n",
      " [  1 166  19   6  11  36   9   3   1   1   0   9  12   6  16   1   5   0\n",
      "    1   0]\n",
      " [  0  23 129  47  15  22   5   4   0   1   0   5  11   8   5   2   1   0\n",
      "    2   0]\n",
      " [  0  22  28 154  22   7  14   0   0   0   1   6  21   4   5   0   2   0\n",
      "    0   0]\n",
      " [  0  31  14  22 159   6   9   2   1   1   0   5  17   4   4   0   0   0\n",
      "    0   0]\n",
      " [  0  42  15   8   8 183   6   4   0   1   1   4   3  10   7   1   3   2\n",
      "    1   0]\n",
      " [  1  11  13  27  22   4 145   7   3   3   1   9  20  12  18   0   3   2\n",
      "    1   0]\n",
      " [  2   2   2   4   4   1  11 182  21   3   0   7  11   7  15   1   9   3\n",
      "    8   0]\n",
      " [  0   0   5   0   1   0   9  35 235   0   0   2   4  19   7   1   4   2\n",
      "    5   0]\n",
      " [  1   0   0   0   0   0   1   3   2 235  15   1   3   8   7   3   7   7\n",
      "   11   1]\n",
      " [  1   0   2   1   0   0   0   0   0  13 274   0   0   2   4   0   3   2\n",
      "    8   0]\n",
      " [  2   6   1   0   0   1   0   0   0   0   0 250   1   0   1   0  14   0\n",
      "    3   0]\n",
      " [  3  12  14  22  16   1  19  11   2   0   1  14 168   6  14   1   4   1\n",
      "    1   0]\n",
      " [  3   2   2   0   5   2   4   6   3   4   2   3   5 219   9   4  16   3\n",
      "   12   5]\n",
      " [  2   2   2   0   1   3   2   2   0   1   1   2   5   6 248   2   5   1\n",
      "    9   0]\n",
      " [  5   1   1   1   1   1   0   0   1   0   1   0   2   3   4 250   5   7\n",
      "    8  15]\n",
      " [  2   0   0   0   0   1   0   0   1   1   1   9   1   2   2   1 197   5\n",
      "   24   7]\n",
      " [  0   1   0   0   1   1   0   0   2   1   0   3   0   1   1   2   2 270\n",
      "    4   3]\n",
      " [  4   0   0   0   0   1   0   0   1   0   2   4   0   1   2   4  22   5\n",
      "  161   2]\n",
      " [ 24   0   0   1   0   0   0   0   1   2   1   2   0   1   5  28  15   3\n",
      "   18  95]]\n",
      "('The accuracy of Random Forest on test data is:', 0.77473013625906917)\n",
      "Classification Metrics for RandomForest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.74      0.77       220\n",
      "          1       0.60      0.68      0.64       303\n",
      "          2       0.63      0.79      0.70       280\n",
      "          3       0.67      0.65      0.66       286\n",
      "          4       0.72      0.73      0.73       275\n",
      "          5       0.69      0.64      0.67       299\n",
      "          6       0.73      0.89      0.80       302\n",
      "          7       0.81      0.79      0.80       293\n",
      "          8       0.90      0.90      0.90       329\n",
      "          9       0.79      0.83      0.81       305\n",
      "         10       0.83      0.94      0.88       310\n",
      "         11       0.87      0.94      0.90       279\n",
      "         12       0.77      0.51      0.62       310\n",
      "         13       0.79      0.75      0.77       309\n",
      "         14       0.86      0.86      0.86       294\n",
      "         15       0.72      0.92      0.81       306\n",
      "         16       0.80      0.88      0.84       254\n",
      "         17       0.96      0.88      0.91       292\n",
      "         18       0.87      0.66      0.75       209\n",
      "         19       0.88      0.35      0.50       196\n",
      "\n",
      "avg / total       0.78      0.77      0.77      5651\n",
      "\n",
      "Confusion matrix\n",
      "[[162   2   0   0   1   2   1   2   1   4   0   1   2   2   2  26   2   1\n",
      "    4   5]\n",
      " [  2 206  19   5  23  16   9   2   0   3   0   2   5   1   6   2   0   1\n",
      "    1   0]\n",
      " [  0  15 221  10   3  15   4   2   1   2   1   1   2   0   2   1   0   0\n",
      "    0   0]\n",
      " [  1  12  33 185  12  10  12   4   1   0   2   3   7   2   1   0   1   0\n",
      "    0   0]\n",
      " [  0   9   4  23 202   7  10   2   1   4   1   0   9   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0  26  55   5   0 191   1   1   1   3   0   3   3   3   4   2   0   0\n",
      "    0   1]\n",
      " [  1   5   3   4   5   1 270   2   0   2   3   1   1   1   1   0   1   0\n",
      "    1   0]\n",
      " [  1   4   1   1   9   2  10 231  13   3   1   2   7   3   2   2   0   0\n",
      "    0   1]\n",
      " [  0   3   0   3   3   1   9   7 295   1   2   0   0   0   2   1   2   0\n",
      "    0   0]\n",
      " [  0   1   1   2   1   5   5   1   0 254  29   1   1   3   0   0   0   0\n",
      "    1   0]\n",
      " [  1   1   0   0   0   1   1   1   0  12 291   0   0   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   4   0   0   0   5   2   0   2   1   0 262   1   0   0   0   0   0\n",
      "    1   1]\n",
      " [  2  26   6  23   9   8  10  14   4   9   6  10 159  13   7   0   2   0\n",
      "    1   1]\n",
      " [  1  17   2   7   3   3  12   4   3   7   2   3   4 231   5   3   1   0\n",
      "    1   0]\n",
      " [  1   7   0   2   4   1   2   2   2   4   4   2   2   6 252   1   0   0\n",
      "    2   0]\n",
      " [  2   5   0   0   2   2   1   2   0   2   1   1   1   3   1 281   1   1\n",
      "    0   0]\n",
      " [  0   2   2   0   2   1   3   2   1   5   1   2   1   1   1   1 224   1\n",
      "    4   0]\n",
      " [  0   0   1   2   0   2   2   3   0   3   3   2   1   8   2   5   1 256\n",
      "    1   0]\n",
      " [  1   0   1   3   2   0   7   3   1   3   1   3   1   5   4   8  28   1\n",
      "  137   0]\n",
      " [ 25   0   0   1   0   2   1   1   2   0   2   2   0   8   1  57  16   7\n",
      "    3  68]]\n",
      "('The accuracy of SGD on test data is:', 0.85666253760396394)\n",
      "Classification Metrics for SGD\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.90      0.77       220\n",
      "          1       0.82      0.71      0.76       303\n",
      "          2       0.64      0.87      0.74       280\n",
      "          3       0.75      0.72      0.73       286\n",
      "          4       0.81      0.80      0.81       275\n",
      "          5       0.92      0.71      0.80       299\n",
      "          6       0.69      0.90      0.79       302\n",
      "          7       0.97      0.82      0.89       293\n",
      "          8       0.91      0.95      0.93       329\n",
      "          9       0.98      0.89      0.93       305\n",
      "         10       0.94      0.99      0.97       310\n",
      "         11       0.92      0.96      0.94       279\n",
      "         12       0.84      0.81      0.82       310\n",
      "         13       0.93      0.91      0.92       309\n",
      "         14       0.95      0.95      0.95       294\n",
      "         15       0.93      0.87      0.90       306\n",
      "         16       0.89      0.90      0.90       254\n",
      "         17       0.96      0.93      0.95       292\n",
      "         18       0.90      0.85      0.87       209\n",
      "         19       0.81      0.62      0.70       196\n",
      "\n",
      "avg / total       0.87      0.86      0.86      5651\n",
      "\n",
      "Confusion matrix\n",
      "[[197   0   1   0   0   0   2   0   0   0   0   1   0   1   1   4   0   2\n",
      "    2   9]\n",
      " [  3 214  29   8   9  13  10   0   1   1   2   1   4   1   2   0   3   1\n",
      "    0   1]\n",
      " [  2   6 244  13   2   1   8   0   0   1   0   1   1   0   0   0   0   0\n",
      "    0   1]\n",
      " [  4   5  23 205  16   2  17   0   0   1   1   2   8   2   0   0   0   0\n",
      "    0   0]\n",
      " [  1   3   9  19 220   1  12   0   1   0   0   4   2   1   1   0   0   1\n",
      "    0   0]\n",
      " [  4  21  39   2   3 213   4   1   0   0   1   4   1   2   4   0   0   0\n",
      "    0   0]\n",
      " [  0   1   3  10   6   0 273   0   2   0   1   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  5   0   3   1   3   0  11 241  14   1   0   1  11   2   0   0   0   0\n",
      "    0   0]\n",
      " [  1   0   1   1   0   0   7   2 311   1   1   0   3   0   0   0   0   0\n",
      "    0   1]\n",
      " [  3   0   5   0   1   0   9   0   0 272   9   0   2   0   0   1   0   0\n",
      "    1   2]\n",
      " [  0   0   0   0   0   0   1   0   2   0 307   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   1   0   0   0   2   0   1   0   0 268   1   0   1   0   1   0\n",
      "    1   0]\n",
      " [  2   4   6  13   6   2  14   2   5   0   1   3 250   1   0   0   0   1\n",
      "    0   0]\n",
      " [  4   3   4   1   1   0   7   0   3   0   1   0   2 281   0   0   1   0\n",
      "    0   1]\n",
      " [  5   0   0   0   0   0   3   0   2   0   0   0   1   1 280   0   0   1\n",
      "    1   0]\n",
      " [ 15   0   5   0   0   0   0   0   0   0   1   1   2   3   2 265   1   1\n",
      "    0  10]\n",
      " [  1   0   3   1   0   0   2   0   1   0   1   3   1   1   1   0 228   1\n",
      "    9   1]\n",
      " [  3   0   0   0   3   0   5   1   0   1   0   0   2   0   0   1   0 273\n",
      "    3   0]\n",
      " [  3   0   1   0   0   0   5   1   0   0   0   2   0   2   0   1  13   1\n",
      "  178   2]\n",
      " [ 38   0   2   0   0   0   1   1   0   0   0   0   0   3   2  14   8   3\n",
      "    3 121]]\n"
     ]
    }
   ],
   "source": [
    "# Printing out the classification metric for GNB, Random Forest and Stochastic gradient descent SVM\n",
    "obj.test_performance(test_X.toarray(),test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Finding the descrambled Words from a list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of words list, and a word list, we need to find all the words in the world list which are descramble\n",
    "of the word in scrambled word list \n",
    "\n",
    "e.g. : Scrambled list: ['dgo']  \n",
    "       Wordlist=['dog','god','ddgoi']  \n",
    "    >> print---> 'dog' 'god'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "god\n",
      "nam\n"
     ]
    }
   ],
   "source": [
    "# function to find descrambled words\n",
    "\n",
    "def descramble(scrambled_list,word_list):\n",
    "    \"\"\"This function takes a word list (word_list) and \n",
    "    checks if a scrambled word in scrambled list (scrambled_list) is present in the word list\n",
    "    \n",
    "    eg. Scrambled list: ['dgo']\n",
    "        Wordlist=['dog','god','ddgoi']\n",
    "        >>> 'dog' 'god'\n",
    "    \"\"\"\n",
    "    for x in scrambled_list:\n",
    "        for y in word_list:\n",
    "            temp=y\n",
    "            c=0\n",
    "            for z in x:\n",
    "                if(z in temp):\n",
    "                    c+=1\n",
    "                    temp=list(temp)\n",
    "                    temp.remove(z)\n",
    "                    temp=\"\".join(temp)\n",
    "                    if(c==len(x) and (len(x)==len(y))):\n",
    "                        print(y)\n",
    "                else: \n",
    "                    break\n",
    "\n",
    "                    \n",
    "check=['dgo','man','ddii']\n",
    "wordlist=['dog','god','ddgoi','nam','dddi']\n",
    "descramble(check,wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
