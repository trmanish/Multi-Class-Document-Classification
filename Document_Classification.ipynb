{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does Multi-Class Classification on 20 sets of Documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import scipy.sparse as sp\n",
    "%matplotlib inline  \n",
    "# Don't use plt.show() as it opens a new window and blocks the evaluation of cell. \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing the default data frame options. \n",
    "pd.set_option(\"max_r\",3000) # Set the default rows to 80\n",
    "pd.set_option(\"max_columns\",51000) # Set the default columns to 500 from 20. \n",
    "pd.set_option(\"expand_frame_repr\",False) # Checks if frame can be expanded or truncated. Make it expand column-wise\n",
    "np.set_printoptions(threshold=np.inf) # Removes the threshold level to print numpy array\n",
    "np.set_printoptions(suppress=True) # Prevent printing in scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multiclass Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset from different folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This funtion reads in the data from different documents and stores as list\n",
    "\n",
    "def readingData():\n",
    "    \"\"\"This fucntion reads the document files from different sub-directories\"\"\"\n",
    "    docs_list=[]\n",
    "    target_class=[]\n",
    "    target_recoded=[]\n",
    "    x=-1\n",
    "    for (dirnames, dir,files) in os.walk('./data'):\n",
    "        if (x<0):\n",
    "            class_names=dir\n",
    "            x+=1\n",
    "            continue\n",
    "        for y in range(len(files)):\n",
    "            docs_list.append(open(dirnames+'/'+files[y]).read())\n",
    "            target_class.append(dirnames[7:])\n",
    "            target_recoded.append(x)    # Getting the target class as numeric codes: 0 for 1st class, 1 for 2nd and so on..\n",
    "        x+=1\n",
    "    \n",
    "    # Taking some sense of data\n",
    "    print(\"Total no. of documents:\",len(target_class))\n",
    "    print(\"Total Classes:\", len(class_names))\n",
    "    print(\"Average no. of documents in a class\",len(target_class)/len(class_names))\n",
    "    return docs_list,target_recoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training and test split\n",
    "\n",
    "def split(docs_list,target_recoded):\n",
    "    \"\"\"This function samples the dataset into training and testing\"\"\"\n",
    "    # Splitting into training and test. \n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    train_X, test_X,train_Y,test_Y = train_test_split(docs_list, target_recoded, test_size=0.30, random_state=42)\n",
    "    \n",
    "    return train_X, test_X,train_Y,test_Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cleaning the doc files\n",
    "\n",
    "def cleaningDocs(doc,stem='L'):  # 'S' for Stemming, 'L' for Lemmatization\n",
    "    \"\"\"This function cleans each doc string by doing the following: \n",
    "    i)   Removing punctuation and other non alphabetical characters\n",
    "    ii)  Convert to Lower case and split string into words (tokenization)\n",
    "    ii)  Removes stop words (most frequent words)\n",
    "    iii) Doing Stemming and Lemmatization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing punctuations and other non alphabetic characters\n",
    "    import re\n",
    "    alphabets_only=re.sub(r'[^a-zA-Z]',\" \",doc)\n",
    "    \n",
    "    # Converting to lower case and splitting the words(tokenization)\n",
    "    words_lower=alphabets_only.lower().split()\n",
    "    \n",
    "    # Removing stop words (Words like 'a', 'an', 'is','the' which doesn't contribute anything\n",
    "    from nltk.corpus import stopwords\n",
    "    useful_words = [w for w in words_lower if not w in set(stopwords.words(\"english\"))] \n",
    "    \n",
    "    # Doing Stemming or Lemmatization (Normalising the text)\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    if (stem=='S'):  # Choosing between Stemming ('S') and Lemmatization ('L')\n",
    "        stemmer=PorterStemmer()\n",
    "        final_words=[stemmer.stem(x) for x in useful_words]\n",
    "    else: \n",
    "        lemma=WordNetLemmatizer()\n",
    "        final_words=[lemma.lemmatize(x) for x in useful_words]\n",
    "        \n",
    "    \n",
    "    return( \" \".join(final_words))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def processing(file,stem=\"L\"):\n",
    "    \"\"\" Function to clean the training or test docs\n",
    "    Pass the name of file as argument to be cleaned\n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_train_X=[]\n",
    "    for x in file: \n",
    "        cleaned_train_X.append(cleaningDocs(x,stem))\n",
    "    return cleaned_train_X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating Bag of words feature vectors for each document\n",
    "\n",
    "def bagofWords(X,Y,max_feature=5000,type=\"count\"):\n",
    "    \"\"\"This function creates a Bag of Features vectors from the original documents\"\"\"\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    \n",
    "    if(type==\"count\"): # To choose between count or tf-idf bag or words model\n",
    "        vectorizer = CountVectorizer(analyzer = \"word\",max_features = max_feature) \n",
    "    else: \n",
    "        vectorizer = TfidfVectorizer(analyzer = \"word\",max_features = max_feature)\n",
    "        \n",
    "    X=vectorizer.fit_transform(X)\n",
    "    return X ,np.array(Y)  # Converting to numpy array\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class learner(object):\n",
    "    \"\"\"Creating a class to efficiently run multiple algorithms on the same dataset\"\"\"\n",
    "    \n",
    "    def __init__(self,train_X,train_Y,k=5):\n",
    "        self.k=k\n",
    "        self.train_X=train_X\n",
    "        self.train_Y=train_Y\n",
    "\n",
    "        \n",
    "    # Running algorithm with 5 fold cross-validation\n",
    "\n",
    "    def kFold(self):\n",
    "        '''This fucntion splits the training set into k folds\n",
    "        '''\n",
    "        from sklearn import cross_validation\n",
    "        self.k_fold=cross_validation.KFold(n=self.train_X.shape[0],n_folds=self.k)\n",
    "    \n",
    "\n",
    "            \n",
    "    def GNB(self):\n",
    "        \"\"\"Method to implement Multi-class Gaussian Naive Bayes\"\"\"\n",
    "        \n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        scores_gnb = []\n",
    "        \n",
    "        for train_indices, test_indices in self.k_fold:\n",
    "            train_X_cv = self.train_X[train_indices].todense()\n",
    "            train_Y_cv= self.train_Y[train_indices]\n",
    " \n",
    "            test_X_cv = self.train_X[test_indices].todense()\n",
    "            test_Y_cv= self.train_Y[test_indices]\n",
    "        \n",
    "            self.gnb=GaussianNB()\n",
    "            scores_gnb.append(self.gnb.fit(train_X_cv,train_Y_cv).score(test_X_cv,test_Y_cv))\n",
    "            \n",
    "        print(\"The mean accuracy of GaussianNaive Bayes on CV data is:\", np.mean(scores_gnb))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def RF(self):\n",
    "        \"\"\"Method to implement Multi-class RandomForest\"\"\"\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        scores_rf = []\n",
    "        \n",
    "        for train_indices, test_indices in self.k_fold:\n",
    "            train_X_cv = self.train_X[train_indices].todense()\n",
    "            train_Y_cv= self.train_Y[train_indices]\n",
    " \n",
    "            test_X_cv = self.train_X[test_indices].todense()\n",
    "            test_Y_cv= self.train_Y[test_indices]\n",
    "        \n",
    "            self.rf=RandomForestClassifier(n_estimators=150,criterion='entropy')\n",
    "            scores_rf.append(self.rf.fit(train_X_cv,train_Y_cv).score(test_X_cv,test_Y_cv))\n",
    "\n",
    "        print(\"The mean accuracy of Random Forests on CV data is:\", np.mean(scores_rf))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def SGD(self):\n",
    "        \"\"\"Method to implement Multi-class SVM using Stochastic Gradient Descent\"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        scores_sgd = []\n",
    "        \n",
    "        for train_indices, test_indices in self.k_fold:\n",
    "            train_X_cv = self.train_X[train_indices].todense()\n",
    "            train_Y_cv= self.train_Y[train_indices]\n",
    " \n",
    "            test_X_cv = self.train_X[test_indices].todense()\n",
    "            test_Y_cv= self.train_Y[test_indices]\n",
    "        \n",
    "            self.sgd=SGDClassifier(loss='hinge',penalty='l2')\n",
    "            scores_sgd.append(self.sgd.fit(train_X_cv,train_Y_cv).score(test_X_cv,test_Y_cv))\n",
    "\n",
    "        print(\"The mean accuracy of Stochastic Gradient Descent Classifier on CV data is:\", np.mean(scores_sgd))   \n",
    "        \n",
    "        \n",
    "        \n",
    "    def test_performance(self,test_X,test_Y):\n",
    "        \"\"\"This method checks the performance of each algorithm on test data.\"\"\"\n",
    "        \n",
    "        from sklearn import metrics\n",
    "        \n",
    "        # For GNB\n",
    "        print (\"The accuracy of GNB on test data is:\", self.gnb.score(test_X,test_Y))\n",
    "        print 'Classification Metrics for GNB'\n",
    "        print metrics.classification_report(test_Y, self.gnb.predict(test_X))\n",
    "        print \"Confusion matrix\"\n",
    "        print metrics.confusion_matrix(test_Y, self.gnb.predict(test_X))\n",
    "        \n",
    "        # For RandomForest\n",
    "        print (\"The accuracy of Random Forest on test data is:\", self.rf.score(test_X,test_Y))\n",
    "        print 'Classification Metrics for RandomForest'\n",
    "        print metrics.classification_report(test_Y, self.rf.predict(test_X))\n",
    "        print \"Confusion matrix\"\n",
    "        print metrics.confusion_matrix(test_Y, self.rf.predict(test_X))\n",
    "        \n",
    "        # For SGD\n",
    "        print (\"The accuracy of SGD on test data is:\", self.sgd.score(test_X,test_Y))\n",
    "        print 'Classification Metrics for SGD'\n",
    "        print metrics.classification_report(test_Y, self.sgd.predict(test_X))\n",
    "        print \"Confusion matrix\"\n",
    "        print metrics.confusion_matrix(test_Y, self.sgd.predict(test_X))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning original docs by doing text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total no. of documents:', 18834)\n",
      "('Total Classes:', 20)\n",
      "('Average no. of documents in a class', 941)\n"
     ]
    }
   ],
   "source": [
    "# Reading Datasets\n",
    "docs_list,target_Y=readingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into training and test\n",
    "train_X, test_X,train_Y,test_Y=split(docs_list,target_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning the training docs files\n",
    "cleaned_train_X=processing(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating bag of words\n",
    "train_X, train_Y=bagofWords(cleaned_train_X,train_Y,type=\"tfid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Runnning the algorithms\n",
    "obj=learner(train_X,train_Y)\n",
    "obj.kFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The mean accuracy of GaussianNaive Bayes on CV data is:', 0.6906625280601778)\n"
     ]
    }
   ],
   "source": [
    "# Running Gaussian Naive Bayes\n",
    "obj.GNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The mean accuracy of Random Forests on CV data is:', 0.76704899288346129)\n"
     ]
    }
   ],
   "source": [
    "#Running Random Forests\n",
    "obj.RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The mean accuracy of Stochastic Gradient Descent Classifier on CV data is:', 0.85739335693812158)\n"
     ]
    }
   ],
   "source": [
    "# Running Stochastic Gradient Descent SVM\n",
    "obj.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting the test data into Bags of model. \n",
    "\n",
    "# Cleaning the training docs files\n",
    "test_X=processing(test_X)\n",
    "# Creating bag of words\n",
    "test_X, test_Y=bagofWords(test_X,test_Y,type=\"tfid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The accuracy of GNB on test data is:', 0.061582020881259955)\n",
      "Classification Metrics for GNB\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.03      0.03      0.03       220\n",
      "          1       0.09      0.05      0.06       303\n",
      "          2       0.06      0.05      0.05       280\n",
      "          3       0.15      0.03      0.06       286\n",
      "          4       0.00      0.00      0.00       275\n",
      "          5       0.06      0.07      0.06       299\n",
      "          6       0.04      0.01      0.01       302\n",
      "          7       0.04      0.01      0.02       293\n",
      "          8       0.18      0.05      0.08       329\n",
      "          9       0.02      0.00      0.01       305\n",
      "         10       0.11      0.03      0.04       310\n",
      "         11       0.04      0.07      0.05       279\n",
      "         12       0.07      0.04      0.05       310\n",
      "         13       0.06      0.14      0.08       309\n",
      "         14       0.08      0.20      0.12       294\n",
      "         15       0.10      0.09      0.10       306\n",
      "         16       0.06      0.14      0.09       254\n",
      "         17       0.06      0.09      0.08       292\n",
      "         18       0.03      0.11      0.05       209\n",
      "         19       0.05      0.03      0.03       196\n",
      "\n",
      "avg / total       0.07      0.06      0.05      5651\n",
      "\n",
      "Confusion matrix\n",
      "[[ 7  6  7  1  0 10  0  5  3  0  0 32  1 24  7 24 20 24 41  8]\n",
      " [16 14 17  2  1 20  1  3  5  4  3 34  3 29 41 23 18 27 35  7]\n",
      " [11  6 13  5  6 23  2  6  3  1  2 29  4 32 51 16 33  9 20  8]\n",
      " [ 9 11 10 10  4 10  2  6  2  3  7 28 12 30 35 16 44 15 28  4]\n",
      " [ 5 10 14  6  0 15  7  6  1  1  7 18 23 46 34  7 28 12 29  6]\n",
      " [12 18 11  3  2 20  5 17  5  4  4 25  6 36 29 16 25 20 38  3]\n",
      " [14  5  8  0  3 16  2  2  5  2  7 31 13 34 31 11 38 33 38  9]\n",
      " [11  1 11  3  2 25  2  4  9  5  1 23 18 36 39  5 30 30 31  7]\n",
      " [12  5 19  7  5 29  3 11 17  3  6 59 14 36 29  7 17 22 25  3]\n",
      " [11  6 18  3  1 13  2  2  7  1  8 29  8 28 34 23 33 27 46  5]\n",
      " [ 9  6 15  0  2 25  7  8  4  7  8 33 17 26 58 14 22 23 25  1]\n",
      " [ 4 10  3  1  0 14  5  1  4  1  0 20 11 24 24 10 32 61 50  4]\n",
      " [12  7 14  6  2 20  4 10  8  2  4 25 12 42 60 13 15 18 32  4]\n",
      " [15 18  9  1  0 13  5  5  3  2  4 30 12 42 58 13 31 16 31  1]\n",
      " [16  6  5  3  6 25  0  8  3  2  2 15  5 48 58 17 28 12 34  1]\n",
      " [21  4  9  0  0 19  2  7  4  1  3 22  7 49 27 29 40 16 30 16]\n",
      " [ 9  4  1  0  0  4  0  2  2  5  1 51  3 66 20  7 36 12 29  2]\n",
      " [33  7  5 14  0 10  0  5  2  3  3 18  0 29 28 20 25 27 59  4]\n",
      " [17  3  5  0  1 12  2  2  1  2  2 30  4 32 20 17 21  9 23  6]\n",
      " [14  2  9  0  0  5  0  1  5  1  1 14  3 37 15 14 24 13 33  5]]\n",
      "('The accuracy of Random Forest on test data is:', 0.088656874889400106)\n",
      "Classification Metrics for RandomForest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.07      0.01      0.02       220\n",
      "          1       0.05      0.02      0.03       303\n",
      "          2       0.04      0.02      0.03       280\n",
      "          3       0.04      0.02      0.03       286\n",
      "          4       0.03      0.03      0.03       275\n",
      "          5       0.07      0.78      0.13       299\n",
      "          6       0.26      0.36      0.30       302\n",
      "          7       0.02      0.00      0.01       293\n",
      "          8       0.10      0.03      0.04       329\n",
      "          9       0.38      0.22      0.28       305\n",
      "         10       0.03      0.04      0.03       310\n",
      "         11       0.04      0.01      0.02       279\n",
      "         12       0.00      0.00      0.00       310\n",
      "         13       0.09      0.04      0.05       309\n",
      "         14       0.05      0.01      0.02       294\n",
      "         15       0.01      0.01      0.01       306\n",
      "         16       0.16      0.04      0.06       254\n",
      "         17       0.25      0.05      0.08       292\n",
      "         18       0.00      0.00      0.00       209\n",
      "         19       0.00      0.00      0.00       196\n",
      "\n",
      "avg / total       0.09      0.09      0.06      5651\n",
      "\n",
      "Confusion matrix\n",
      "[[  2   4   8   2   8 132  11   5   9   4  16   4   0   7   2   2   3   0\n",
      "    1   0]\n",
      " [  0   7   9   3   8 233  15   1   0   2   9   1   1   2   2   7   0   3\n",
      "    0   0]\n",
      " [  3   8   6   7   6 208   8   1   1   1   7   1   1   1   4  14   3   0\n",
      "    0   0]\n",
      " [  1   4   3   6   3 197  16   0   1   2  11   2   0   3   4  29   3   1\n",
      "    0   0]\n",
      " [  1   6   4   2   7 175  17   1   1   3  10   4   0   4   3  35   2   0\n",
      "    0   0]\n",
      " [  2  10   8   5   9 232  13   4   1   1   8   1   0   0   1   2   2   0\n",
      "    0   0]\n",
      " [  2   2   2   4   8 151 109   0   1   2   5   1   0   4   2   5   3   1\n",
      "    0   0]\n",
      " [  1   7   7   6  14 175  16   1  11   8   8   6   1   7   8   8   1   4\n",
      "    4   0]\n",
      " [  1  11  10   9   8 195  33   0   9  10  13   2   2  11   5   2   3   4\n",
      "    1   0]\n",
      " [  1   0   4   3  29 134   8   1   8  66  31   3   1  11   2   1   1   1\n",
      "    0   0]\n",
      " [  1   6   6  22  18 154   8  15   2  28  11  13   0   2   4   7   5   8\n",
      "    0   0]\n",
      " [  1   9   8   4  11  87  16   1   1   4  15   4   1   2   3 107   3   2\n",
      "    0   0]\n",
      " [  0  13   5   2  11 192  16   0   2   2  10   1   0   7   7  38   1   2\n",
      "    1   0]\n",
      " [  3  10  13   4  20 184  16   0   0   3  19   6   8  11   1   5   5   1\n",
      "    0   0]\n",
      " [  2   6   2  33  13 114  52   3   4   5  30   8   0   6   4  11   0   0\n",
      "    1   0]\n",
      " [  5   3  16   5   9 147   8   3  25   4  53   8   0   6  10   2   2   0\n",
      "    0   0]\n",
      " [  1  12  11   8   6 110  16   1   1  11  16   1   1  19   1  18  10  11\n",
      "    0   0]\n",
      " [  0  10   8   4   7 142   9   4   1  10  31  13   1  14  11   6   7  14\n",
      "    0   0]\n",
      " [  0  11   6   3  17  98  22   3   0   3  20   9   0   5   2   3   5   2\n",
      "    0   0]\n",
      " [  0   3   8   4   8 109   6   2  13   3  18   8   0   4   3   2   2   3\n",
      "    0   0]]\n",
      "('The accuracy of SGD on test data is:', 0.092549991152008487)\n",
      "Classification Metrics for SGD\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.09      0.33      0.14       220\n",
      "          1       0.06      0.02      0.03       303\n",
      "          2       0.06      0.14      0.09       280\n",
      "          3       0.07      0.04      0.05       286\n",
      "          4       0.09      0.04      0.06       275\n",
      "          5       0.13      0.05      0.08       299\n",
      "          6       0.11      0.54      0.19       302\n",
      "          7       0.19      0.03      0.05       293\n",
      "          8       0.07      0.09      0.08       329\n",
      "          9       0.29      0.08      0.13       305\n",
      "         10       0.10      0.08      0.09       310\n",
      "         11       0.10      0.05      0.06       279\n",
      "         12       0.07      0.06      0.06       310\n",
      "         13       0.07      0.06      0.06       309\n",
      "         14       0.05      0.03      0.04       294\n",
      "         15       0.05      0.01      0.02       306\n",
      "         16       0.14      0.06      0.08       254\n",
      "         17       0.09      0.08      0.08       292\n",
      "         18       0.02      0.00      0.01       209\n",
      "         19       0.09      0.09      0.09       196\n",
      "\n",
      "avg / total       0.10      0.09      0.07      5651\n",
      "\n",
      "Confusion matrix\n",
      "[[ 73  14  28   7   4   5  31   3  14   0  13   3   4   7   5   1   5   2\n",
      "    1   0]\n",
      " [ 44   6  36   7   8   5  61   2  15  16   5   3  12  12  13   7   4  37\n",
      "    1   9]\n",
      " [ 53   7  39  12  15   4  44   0  15   3  13   2  11  20   6   9  11   7\n",
      "    6   3]\n",
      " [ 34  10  32  12   9   8  70   2   9   1  13   4  13   6  20   9  13  13\n",
      "    0   8]\n",
      " [ 17   5  44   8  11   6  63   0   9   1   4   7  22   9  33   7   2  16\n",
      "    3   8]\n",
      " [ 42  10  33   9   6  16  75   3  34   7   6   9  12   9  14   0   1   7\n",
      "    2   4]\n",
      " [ 18   6  15   8   5   6 162   3  10   2   5   3  12   9  12   3   4  16\n",
      "    1   2]\n",
      " [ 36   2  38   3   4   5  80   8  13   4   6   7  18  18  10   4   1  11\n",
      "    6  19]\n",
      " [ 44   5  14   3   6   3 130   0  28   6  15  10  23  12   4   2   1  18\n",
      "    0   5]\n",
      " [ 37   3  27  13   1   2  60   0  29  25  25   3   7  18  18   3   3  13\n",
      "   15   3]\n",
      " [ 28   2  26  45   0   3  72   7  30   8  26   3   8  10   5   1  14   8\n",
      "    2  12]\n",
      " [ 32   6  49   2   3   2  66   2  21   2  14  13   8   5  11  12   7   7\n",
      "    1  16]\n",
      " [ 30   4  41   9   8  10  83   1  21   3  10   2  18  17   8   4   6  15\n",
      "    0  20]\n",
      " [ 35   3  26   4  13   9  84   0  41   3   7  10  28  17   5   2   1  10\n",
      "    4   7]\n",
      " [ 38   1  24   5   3   1 119   0  20   0  16   3   7  16  10   2   3  16\n",
      "    2   8]\n",
      " [ 67   3  81   3   5   8  35   4  18   0  21   5  13  13  12   4   2   2\n",
      "    2   8]\n",
      " [ 26   1  16   1   1   3  67   0  11   0  30   9  11  22   2   3  15   9\n",
      "    3  24]\n",
      " [ 85   0  30  13   4   6  37   4  11   2   9  12  16  15   6   0   7  22\n",
      "    7   6]\n",
      " [ 36   0   5   0  11  11  44   3   9   1  14  11  10  15   9   1   9  10\n",
      "    1   9]\n",
      " [ 32   7  33   3   1   7  28   0  16   1   9   8   2   9   5   3   1   9\n",
      "    5  17]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manishtripathi/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Printing out the classification metric for GNB, Random Forest and Stochastic gradient descent SVM\n",
    "obj.test_performance(test_X.toarray(),test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Finding the descrambled Words from a list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of words list, and a word list, we need to find all the words in the world list which are descramble\n",
    "of the word in scrambled word list \n",
    "\n",
    "e.g. : Scrambled list: ['dgo']  \n",
    "       Wordlist=['dog','god','ddgoi']  \n",
    "    >> print---> 'dog' 'god'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "god\n",
      "nam\n"
     ]
    }
   ],
   "source": [
    "# function to find descrambled words\n",
    "\n",
    "def descramble(scrambled_list,word_list):\n",
    "    \"\"\"This function takes a word list (word_list) and \n",
    "    checks if a scrambled word in scrambled list (scrambled_list) is present in the word list\n",
    "    \n",
    "    eg. Scrambled list: ['dgo']\n",
    "        Wordlist=['dog','god','ddgoi']\n",
    "        >>> 'dog' 'god'\n",
    "    \"\"\"\n",
    "    for x in scrambled_list:\n",
    "        for y in word_list:\n",
    "            temp=y\n",
    "            c=0\n",
    "            for z in x:\n",
    "                if(z in temp):\n",
    "                    c+=1\n",
    "                    temp=list(temp)\n",
    "                    temp.remove(z)\n",
    "                    temp=\"\".join(temp)\n",
    "                    if(c==len(x) and (len(x)==len(y))):\n",
    "                        print(y)\n",
    "                else: \n",
    "                    break\n",
    "\n",
    "                    \n",
    "check=['dgo','man','ddii']\n",
    "wordlist=['dog','god','ddgoi','nam','dddi']\n",
    "descramble(check,wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
